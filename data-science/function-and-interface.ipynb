{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator LabelEncoder from version 1.6.1 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = joblib.load('data/trained_model.joblib')\n",
    "vectorizer = joblib.load('data/vectorizer.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    '''Preprocess text by making it lowercase, removing text in square brackets,\n",
    "removing links, removing punctuation, and removing words containing numbers.'''\n",
    "    return re.sub('\\[.*?\\]|\\w*\\d\\w*|https?://\\S+|www\\.\\S+|<.*?>+|[%s]' %\n",
    "re.escape(string.punctuation), '', str(text).lower())\n",
    "\n",
    "def apply_stemming(sentence):\n",
    "    stemmer = nltk.SnowballStemmer(\"english\")\n",
    "    return ' '.join(stemmer.stem(word) for word in sentence.split(' '))\n",
    "\n",
    "def preprocess_and_clean(sentence):\n",
    "    '''Preprocess and clean the text'''\n",
    "    cleaned_text = preprocess_text(sentence)\n",
    "    stop_words = stopwords.words('english')\n",
    "    removed_stopwords_text = ' '.join(word for word in\n",
    "cleaned_text.split(' ') if word not in stop_words)\n",
    "    stemmed_text = ' '.join(apply_stemming(word) for word\n",
    "in removed_stopwords_text.split(' '))\n",
    "    return stemmed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def returnClassifyResults(txt: str, model, vectorizer, k:int = 3) -> np.ndarray:\n",
    "    \"\"\"\n",
    "        Function to return top k classification results based on given text\n",
    "    \"\"\"\n",
    "    processed_txt = preprocess_and_clean(txt)\n",
    "    x = pd.DataFrame([processed_txt], columns = ['preprocessed_text'])\n",
    "    x_vectorized = vectorizer.transform(x['preprocessed_text'])\n",
    "    y_pred_proba = model.predict_proba(x_vectorized)\n",
    "    topk_indices = np.argsort(y_pred_proba[0])[::-1][:k]\n",
    "    topk_categories = model.classes_[topk_indices]\n",
    "    topk_probabilities = y_pred_proba[0][topk_indices]\n",
    "    return topk_categories, topk_probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: health, Probability: 0.9772184712538277\n",
      "Category: society, Probability: 0.019720840618475097\n",
      "Category: economy, business and finance, Probability: 0.001408854446298189\n"
     ]
    }
   ],
   "source": [
    "txt = \"\"\"\n",
    "This year’s flu season is on track to be the worst it’s been since before the Covid-19 pandemic, as respiratory illnesses surge to a second peak. There have already been at least 24 million illnesses, 310,000 hospitalizations, and 13,000 deaths from flu, the US Centers for Disease Control and Prevention estimates. The cumulative hospitalization rate – about 64 stays for every 100,000 people, as of February 1 – is the highest it’s been at this point in the season for the past seven years. Flu activity is high or very high in all but six states.\n",
    "\"\"\"\n",
    "topk_categories, topk_probabilities = returnClassifyResults(txt, model, vectorizer, k=3)\n",
    "\n",
    "for category, probability in zip(topk_categories, topk_probabilities):\n",
    "    print(f\"Category: {category}, Probability: {probability}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pseudoClassifyAPI(txt: str):\n",
    "    \n",
    "    # return top 5 search results\n",
    "    topk_categories, topk_probabilities = returnClassifyResults(txt, model, vectorizer, k=3)\n",
    "    response = {category: probability for category, probability in zip(topk_categories, topk_probabilities)}\n",
    "    # response = {'categories': topk_categories, 'probabilities': topk_probabilities}\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7868\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7868/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "gr.Interface(fn=pseudoClassifyAPI, \n",
    "             inputs='text', \n",
    "             outputs=gr.Label(num_top_classes=3),\n",
    "             theme='ocean',\n",
    "             title = 'Multi-label news category classificatiom').launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "import gradio as gr\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "\n",
    "# Function to fetch a random CNN Lite article\n",
    "def get_random_cnn_article():\n",
    "    base_url = \"https://lite.cnn.com/\"\n",
    "    response = requests.get(base_url)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        return \"Failed to fetch articles.\"\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    # Extract article links\n",
    "    articles = [a[\"href\"] for a in soup.find_all(\"a\", href=True) if a[\"href\"].startswith(\"/\")]\n",
    "    if not articles:\n",
    "        return \"No articles found.\"\n",
    "    \n",
    "    # Pick a random article\n",
    "    random_article = random.choice(articles)\n",
    "    article_url = f\"{base_url}{random_article}\"\n",
    "    \n",
    "    # Fetch article content\n",
    "    response = requests.get(article_url)\n",
    "    if response.status_code != 200:\n",
    "        return \"Failed to fetch the article.\"\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    paragraphs = [p.get_text() for p in soup.find_all(\"p\")]\n",
    "    txt = \"\\n\".join(paragraphs).split('See Full Web Article')[0]  # Limit output to first 5 paragraphs\n",
    "    return txt  # Limit output to first 5 paragraphs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7880\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7880/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gr.Blocks(theme='ocean') as demo:\n",
    "    gr.Markdown(\"# Multi-label news category classification\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        # Left column\n",
    "        with gr.Column(scale=1):\n",
    "            text_input = gr.Textbox(label=\"Enter text for classification\")\n",
    "            random_button = gr.Button(\"Get a random today's CNN article\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                clear_button = gr.Button(\"Clear\")\n",
    "                classify_button = gr.Button(\"Classify\", variant=\"primary\")\n",
    "        \n",
    "        # Right column\n",
    "        with gr.Column(scale=1):\n",
    "            output = gr.Label(num_top_classes=3)\n",
    "    \n",
    "    random_button.click(\n",
    "        fn=get_random_cnn_article,\n",
    "        outputs=text_input\n",
    "    )\n",
    "    \n",
    "    clear_button.click(\n",
    "        fn=lambda: (\"\", None), \n",
    "        outputs=[text_input, output]\n",
    "    )\n",
    "    \n",
    "    classify_button.click(\n",
    "        fn=pseudoClassifyAPI,\n",
    "        inputs=text_input,\n",
    "        outputs=output\n",
    "    )\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"name\":\"news-cateorization\",\"description\":\"Categorization API for Tzu-Jo Hsu\\'s NLP project demo.\"}'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "requests.get('http://35.92.204.37/info').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
